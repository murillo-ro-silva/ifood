{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as pys\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql as s\n",
    "import datetime as dt\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.types as st\n",
    "import logging\n",
    "\n",
    "AWS_ACCESS_KEY=''\n",
    "AWS_SECRET_KEY=''\n",
    "\n",
    "MAX_MEMORY = \"5g\"\n",
    "\n",
    "spark = pys.sql.SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.jars.packages\", \"com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AWS_ACCESS_KEY) \n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", 1200)\n",
    "\n",
    "LOCAL_PATH = \"~/home/where_you_wish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_log():\n",
    "    from sys import stdout\n",
    "    stdout_handler = logging.StreamHandler(stdout)\n",
    "    handlers = [stdout_handler]\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "        handlers=handlers\n",
    "    )\n",
    "    return logging.getLogger('JOB')\n",
    "\n",
    "logger = config_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transient to Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_datasets_read_types(sys_args):\n",
    "    \"\"\" param sys_args: Receive all atributos from Glue UI param.\n",
    "        All datasets with \"--n_\" prefixes are being treating\n",
    "        with datasets and read types.\n",
    "        e.g:\n",
    "            --n_consumer / csv.\n",
    "            --n_order / json\n",
    "            --n_....\n",
    "        return dict with datasets and read types.\n",
    "    \"\"\"\n",
    "    datasets_read_types = []\n",
    "    for liu in enumerate(sys_args):\n",
    "        if '--n_' in liu[1]:\n",
    "            dataset = liu[1][4:]\n",
    "            read_type = sys_args[liu[0]+1]\n",
    "            datasets_read_types.append([dataset, read_type])\n",
    "    return datasets_read_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_result(date_type):\n",
    "    \"\"\" Recover brazilian time.\n",
    "    \"\"\"\n",
    "    if date_type == 'today':\n",
    "        date_now = (\n",
    "            dt.datetime.utcnow() - dt.timedelta(hours=3)).strftime('%Y-%m-%d')\n",
    "        return str(date_now)\n",
    "    if date_type == 'today_timestamp':\n",
    "        date_now = (\n",
    "            dt.datetime.utcnow() - dt.timedelta(hours=3)) \\\n",
    "            .strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return str(date_now)\n",
    "    else:\n",
    "        date_yesternoon = (\n",
    "            dt.datetime.utcnow() - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        return str(date_yesternoon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_read_write(zone, dataset_name, read_type):\n",
    "    \"\"\"\n",
    "        refurn absolute bucket path.\n",
    "    \"\"\"\n",
    "    BASE_PATH_SOURCE = 's3a://ifood-data-architect-test-source'\n",
    "\n",
    "    if zone == 'source':\n",
    "        PATH = BASE_PATH_SOURCE + '/' + dataset_name + '.' + read_type + '.gz'\n",
    "    else:\n",
    "        PATH = LOCAL_PATH + zone + '/' + dataset_name\n",
    "\n",
    "    return PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(zone, dataset_name, read_type):\n",
    "    \"\"\"\n",
    "    e.g:\n",
    "        Read json and csv from source repo, creating\n",
    "        partition column setting now date.\n",
    "        return spark dataframe\n",
    "    \"\"\"\n",
    "    PARTITION_DATE = datetime_result('today')\n",
    "    PATH = path_read_write(zone, dataset_name, read_type)\n",
    "\n",
    "    READ = spark.read \\\n",
    "        .load(PATH, format=read_type, header=True, infer_schema=True) \\\n",
    "        .withColumn('partition_column', sf.lit(PARTITION_DATE))\n",
    "\n",
    "    return READ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(zone, dataset_name, df):\n",
    "    \"\"\" param zone (transient/raw/refined/trusted)\n",
    "        param dataset_name\n",
    "        param df (spark dataframe)\n",
    "        e.g:\n",
    "            Write some data in .parquet extension,\n",
    "            overwriting existing data inside the partition.\n",
    "        return spark dataframe\n",
    "    \"\"\"\n",
    "    PATH = path_read_write(zone, dataset_name, '.parquet')\n",
    "    df.write \\\n",
    "      .mode('overwrite') \\\n",
    "      .partitionBy('partition_column') \\\n",
    "      .parquet(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ['--n_order','json','--n_consumer','csv','--n_status', 'json', '--n_restaurant', 'csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, type_read in args_datasets_read_types(parameters):\n",
    "    \"\"\" Iterate from all datasets and read_types configurable.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info('Load Transient:' + datetime_result('today_timestamp') + ' - Dataset: ' + dataset_name + ' - type_read: ' + type_read)\n",
    "    df = read_dataset('source', dataset_name, type_read)\n",
    "\n",
    "    logger.info('Write Raw: ' + datetime_result('today_timestamp') + ' - Dataset: ' + dataset_name + ' - type_read: ' + type_read)\n",
    "    write_dataset('raw', dataset_name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw to Trusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_result(date_type):\n",
    "    \"\"\" Recover brazilian time.\n",
    "    \"\"\" \n",
    "    if date_type == 'today':\n",
    "        date_now = (dt.datetime.utcnow() - dt.timedelta(hours=3)).strftime('%Y-%m-%d')\n",
    "        return str(date_now)\n",
    "    if date_type == 'today_timestamp':\n",
    "        date_now = (dt.datetime.utcnow() - dt.timedelta(hours=3)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return str(date_now)\n",
    "    else:\n",
    "        date_yesternoon = (dt.datetime.utcnow() - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        return str(date_yesternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_column(df, columns):\n",
    "    \"\"\"\n",
    "    Used to hash columns\n",
    "    :param df: :class:`pyspark.sql.DataFrame` instance\n",
    "    :param columns: list, columns to hash\n",
    "    :return df: :class:`pyspark.sql.DataFrame` instance\n",
    "    \"\"\"\n",
    "    salt = 'QglW6kADoUt8-FXIHQQsj3tK-Vpz6QaZ2DoCCQKEARM='\n",
    "    for column in columns:\n",
    "        df = df.withColumn(column, sf.trim(sf.col(column)))\n",
    "        df = df.withColumn(column, sf.concat(column, sf.lit(salt)))\n",
    "        df = df.withColumn(column, sf.sha2(column, 256))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sf.udf\n",
    "def hash_tel(column):\n",
    "    \"\"\"\n",
    "    Def for anonymized phone\n",
    "    \"\"\"\n",
    "    return int(column) * 5 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_read_write(zone, dataset_name):\n",
    "    \"\"\"\n",
    "    Def mount absolute bucket path.\n",
    "    \"\"\"\n",
    "    BASE_PATH_SOURCE = LOCAL_PATH\n",
    "\n",
    "    PATH = BASE_PATH_SOURCE + '/' + zone + '/' + dataset_name\n",
    "\n",
    "    return PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_dataframe(pk_col, max_col, dataframe):\n",
    "    \"\"\"\n",
    "    Def for deduplication remaining the most recent record\n",
    "    \"\"\"\n",
    "    LAST_VALUE_ORDER = s.Window.partitionBy(\n",
    "        pk_col) \\\n",
    "        .orderBy(sf.col(max_col).desc())\n",
    "\n",
    "    DF_DEDUP = dataframe.dropDuplicates() \\\n",
    "        .withColumn(\"distinct\", sf.row_number().over(LAST_VALUE_ORDER)) \\\n",
    "        .filter(\"distinct = 1\") \\\n",
    "        .drop(\"distinct\")\n",
    "\n",
    "    return DF_DEDUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_quality(actual, previous):\n",
    "    \"\"\"\n",
    "    :param actual: number of records comming from last trusted zone.\n",
    "    :param previous: number of records comming from the wildcard previues trusted zone.\n",
    "    :return: TRUE, if the dataset is OK to be inserted / FALSE, if any problem\n",
    "    related to quality was found.\n",
    "    \"\"\"\n",
    "    # Checking the percentage of drop in the counting\n",
    "    maximum_percentage_of_drop_in_count = 80\n",
    "\n",
    "    logger.info(f\"Quality Message: Actual {actual} - Previous {previous}\")\n",
    "    \n",
    "    if actual < previous:\n",
    "        if ((((actual / previous) - 1) * 100) * -1) >= maximum_percentage_of_drop_in_count:  # noqa\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_trusted(dataframe, repartition, partition, dataset):\n",
    "    \n",
    "    PATH = path_read_write(\"trusted\", dataset)\n",
    "    try:\n",
    "        # Todo, find last dataset wildcard using boto3 for figureout that.\n",
    "        DF_LATEST = spark.read \\\n",
    "            .parquet(PATH + '/partition=' + datetime_result(\"yesterday\"))\n",
    "        latest_v2_count = DF_LATEST.count()\n",
    "    \n",
    "    except Exception:\n",
    "        latest_v2_count = 1\n",
    "    \n",
    "    logger.info(\"Getting the count from the current versions of the trusted\")\n",
    "    latest_count = dataframe.count()\n",
    "    quality_check = checking_quality(latest_count, latest_v2_count)\n",
    "    \n",
    "    if quality_check:\n",
    "        \n",
    "        dataframe.repartition(repartition).write \\\n",
    "        .parquet(PATH + '/partition=' + datetime_result(\"today\"),\n",
    "             mode=\"overwrite\",\n",
    "             partitionBy=partition)\n",
    "    else:\n",
    "        logger.info(f\"Quality Urgent: Dataset: {dataset} are 80% lower. Check it out!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for read all datasets in raw zone\n",
    "RAW_ORDER = spark.read \\\n",
    "    .load(path_read_write(\"raw\", \"order\"), format='parquet') \\\n",
    "    .drop(\"partition_column\", \"customer_name\") \\\n",
    "\n",
    "RAW_STATUS = spark.read \\\n",
    "    .load(path_read_write(\"raw\", \"status\"), format='parquet') \\\n",
    "    .drop(\"partition_column\") \\\n",
    "    .withColumnRenamed(\"created_at\", \"status_created_at\") \\\n",
    "    .withColumnRenamed(\"value\", \"status_value\") \\\n",
    "\n",
    "RAW_CONSUMER = spark.read \\\n",
    "    .load(path_read_write(\"raw\", \"consumer\"), format='parquet') \\\n",
    "    .drop(\"partition_column\", \"customer_name\") \\\n",
    "    .withColumnRenamed(\"created_at\", \"consumer_created_at\") \\\n",
    "    .withColumn(\"customer_phone_number\", hash_tel('customer_phone_number'))\n",
    "\n",
    "RAW_RESTAURANT = spark.read \\\n",
    "    .load(path_read_write(\"raw\", \"restaurant\"), format='parquet') \\\n",
    "    .drop(\"partition_column\") \\\n",
    "    .withColumnRenamed(\"created_at\", \"restaurant_created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trusted_order():\n",
    "\n",
    "    # Join Order + Consumer + Restaurant\n",
    "    DF_PRIMARY = RAW_ORDER \\\n",
    "        .join(RAW_CONSUMER, 'customer_id') \\\n",
    "        .join(RAW_RESTAURANT, RAW_RESTAURANT.id == RAW_ORDER.merchant_id)\n",
    "\n",
    "    # Read Order dataset\n",
    "    DF_TRUSTED_ORDER = dedup_dataframe(\n",
    "        \"customer_id\",\n",
    "        \"order_created_at\",\n",
    "        DF_PRIMARY)\n",
    "\n",
    "    # Read Status dataset\n",
    "    DF_TRUSTED_STATUS = dedup_dataframe(\n",
    "        \"order_id\",\n",
    "        \"status_created_at\",\n",
    "        RAW_STATUS) \\\n",
    "        .select(\"order_id\", \"status_created_at\", \"status_value\")\n",
    "\n",
    "    # Joining\n",
    "    DF_TRUSTED_FINAL = DF_TRUSTED_ORDER \\\n",
    "        .join(DF_TRUSTED_STATUS, \"order_id\") \\\n",
    "        .withColumn(\"partition\",\n",
    "                    sf.lit(datetime_result(\"today\")))\n",
    "\n",
    "    DF_TRUSTED_ANONYMIZED = hash_column(DF_TRUSTED_FINAL, ['cpf'])\n",
    "\n",
    "    # Writing DF in trusted zone.\n",
    "    write_trusted(DF_TRUSTED_ANONYMIZED, 30, \"partition\", \"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    PATH_ORDER = path_read_write(\"trusted\", \"order\")\n",
    "\n",
    "    DF_TRUSTED = spark.read \\\n",
    "        .parquet(PATH_ORDER)\n",
    "\n",
    "    # Recovering dynamic schema\n",
    "    JSON_SCHEMA = spark.read \\\n",
    "        .json(DF_TRUSTED.rdd.map(lambda row: row.items)).schema\n",
    "\n",
    "    # Inferring schema in string column, to become array para usar explode code\n",
    "    JSON_SCHEMA = st.ArrayType(JSON_SCHEMA)\n",
    "    DF_FLATTEN = DF_TRUSTED \\\n",
    "        .withColumn(\"items\", sf.from_json(\"items\", JSON_SCHEMA))\n",
    "\n",
    "    # Exploding and struturing Items for construct Order Items\n",
    "    DF_EXPLODE_ITEMS = DF_FLATTEN.select(\n",
    "        \"order_id\",\n",
    "        sf.explode(\"items\").alias(\"items\"))\n",
    "\n",
    "    # Structuring data\n",
    "    DF_EXPLODE_ITEMS_DETAILS = DF_EXPLODE_ITEMS \\\n",
    "        .select(\"order_id\",\n",
    "                sf.col(\"items.name\").alias(\"name\"),\n",
    "                sf.col(\"items.addition\")\n",
    "                .getItem('currency').alias(\"addition_currency\"),\n",
    "                sf.col(\"items.addition\").getItem('value').alias(\"addition\"),\n",
    "                sf.col(\"items.discount\")\n",
    "                .getItem('currency').alias(\"discount_currency\"),\n",
    "                sf.col(\"items.discount\").getItem('value').alias(\"discount\"),\n",
    "                sf.col(\"items.quantity\").alias(\"quantity\"),\n",
    "                sf.col(\"items.unitPrice\")\n",
    "                .getItem('currency').alias(\"unit_price_currency\"),\n",
    "                sf.col(\"items.unitPrice\").getItem('value').alias(\"unit_price\"),\n",
    "                sf.col(\"items.externalId\").alias(\"external_id\"),\n",
    "                sf.col(\"items.totalValue\")\n",
    "                .getItem('currency').alias(\"total_value_currency\"),\n",
    "                sf.col(\"items.totalValue\")\n",
    "                .getItem('value').alias(\"total_value\"),\n",
    "                sf.col(\"items.customerNote\").alias(\"customer_note\"),\n",
    "                sf.col(\"items.integrationId\").alias(\"integration_id\"),\n",
    "                sf.col(\"items.totalAddition\")\n",
    "                .getItem('currency').alias(\"tota_addition_currency\"),\n",
    "                sf.col(\"items.totalAddition\")\n",
    "                .getItem('value').alias(\"tota_addition\"),\n",
    "                sf.col(\"items.totalDiscount\")\n",
    "                .getItem('currency').alias(\"total_discount_currency\"),\n",
    "                sf.col(\"items.totalDiscount\")\n",
    "                .getItem('value').alias(\"total_discount\"),\n",
    "                sf.explode(\"items.garnishItems.externalId\")\n",
    "                .alias(\"garnish_external_id\"),\n",
    "                sf.col(\"items.garnishItems\").alias(\"garnish_items\")\n",
    "                ) \\\n",
    "        .dropDuplicates()\n",
    "\n",
    "    # Exploding and struturing Garnish for additing Order Items\n",
    "    DF_EXPLODE_GARNISH = DF_EXPLODE_ITEMS_DETAILS.select(\n",
    "        \"order_id\",\n",
    "        \"garnish_external_id\",\n",
    "        sf.explode(\"garnish_items\").alias(\"garnish_items\"))\n",
    "\n",
    "    # Structuring data\n",
    "    DF_EXPLODE_GARNISH_DETAIL = DF_EXPLODE_GARNISH \\\n",
    "        .select(\"order_id\",\n",
    "                sf.col(\"garnish_items.name\").alias(\"garnish_name\"),\n",
    "                sf.col(\"garnish_items.addition\")\n",
    "                .getItem('value').alias(\"garnish_addition\"),\n",
    "                sf.col(\"garnish_items.addition\")\n",
    "                .getItem('currency').alias(\"garnish_addition_currency\"),\n",
    "                sf.col(\"garnish_items.discount\")\n",
    "                .getItem('value').alias(\"garnish_discount\"),\n",
    "                sf.col(\"garnish_items.discount\")\n",
    "                .getItem('currency').alias(\"garnish_discount_currency\"),\n",
    "                sf.col(\"garnish_items.quantity\").alias(\"garnish_quantity\"),\n",
    "                sf.col(\"garnish_items.sequence\").alias(\"garnish_sequence\"),\n",
    "                sf.col(\"garnish_items.unitPrice\")\n",
    "                .getItem('value').alias(\"garnish_unit_price\"),\n",
    "                sf.col(\"garnish_items.unitPrice\")\n",
    "                .getItem('currency').alias(\"garnish_unit_price_currency\"),\n",
    "                sf.col(\"garnish_items.categoryId\")\n",
    "                .alias(\"garnish_category_id\"),\n",
    "                sf.col(\"garnish_items.categoryName\")\n",
    "                .alias(\"garnish_category_name\"),\n",
    "                sf.col(\"garnish_items.externalId\")\n",
    "                .alias(\"garnish_external_id\"),\n",
    "                sf.col(\"garnish_items.totalValue\")\n",
    "                .getItem('value').alias(\"garnish_total_value\"),\n",
    "                sf.col(\"garnish_items.totalValue\")\n",
    "                .getItem('currency').alias(\"garnish_total_value_currency\"),\n",
    "                sf.col(\"garnish_items.integrationId\")\n",
    "                .alias(\"garnish_integration_id\"),\n",
    "                ) \\\n",
    "        .dropDuplicates()\n",
    "\n",
    "    # Union itens with garnish dataframe,\n",
    "    # when 1 or more item can have 1 or more garnish.\n",
    "    DF_UNION_ITENS_GARNISH = DF_EXPLODE_ITEMS_DETAILS \\\n",
    "        .join(DF_EXPLODE_GARNISH_DETAIL, [\"order_id\", \"garnish_external_id\"]) \\\n",
    "        .drop(\"garnish_items\") \\\n",
    "        .withColumn(\"partition\",\n",
    "            sf.lit(datetime_result(\"today\")))\n",
    "\n",
    "    # Writing DF in trusted zone.\n",
    "    write_trusted(DF_UNION_ITENS_GARNISH, 10, \"partition\", \"order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trusted_order_status():\n",
    "\n",
    "    PATH_ORDER = path_read_write(\"trusted\", \"order\")\n",
    "    DF_ORDER_STATUSES = spark.read \\\n",
    "        .parquet(PATH_ORDER) \\\n",
    "\n",
    "    # Creating temp view in memory for use in query below\n",
    "    DF_ORDER_STATUSES.createOrReplaceTempView(\"order\")\n",
    "    RAW_STATUS.createOrReplaceTempView(\"status\")\n",
    "\n",
    "    DF_STATUSES_ORDER = spark.sql(\"\"\"\n",
    "    select  ord.order_id,\n",
    "            st1.status_value status_1,\n",
    "            st1.status_created_at status_created_at_1,\n",
    "            st2.status_value status_2,\n",
    "            st2.status_created_at status_created_at_2,\n",
    "            st3.status_value status_3,\n",
    "            st3.status_created_at status_created_at_3,\n",
    "            st4.status_value status_4,\n",
    "            st4.status_created_at status_created_at_4\n",
    "    from order ord\n",
    "    left join (\n",
    "        select status_created_at, status_value, order_id\n",
    "          from status\n",
    "         where status_value = 'REGISTERED') st1 on st1.order_id = ord.order_id\n",
    "\n",
    "    left join (\n",
    "        select status_created_at, status_value, order_id\n",
    "          from status\n",
    "         where status_value = 'PLACED') st2 on st2.order_id = ord.order_id\n",
    "\n",
    "    left join (\n",
    "        select status_created_at, status_value, order_id\n",
    "          from status\n",
    "         where status_value = 'CONCLUDED') st3 on st3.order_id = ord.order_id\n",
    "\n",
    "    left join (\n",
    "        select status_created_at, status_value, order_id\n",
    "          from status\n",
    "         where status_value = 'CANCELLED') st4 on st4.order_id = ord.order_id\n",
    "    \"\"\").dropDuplicates() \\\n",
    "    .withColumn(\"partition\", sf.lit(datetime_result(\"today\")))\n",
    "\n",
    "    # Writing DF in trusted zone.\n",
    "    write_trusted(DF_STATUSES_ORDER, 1, \"partition\", \"order_statuses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trusted_order()\n",
    "trusted_order_items()\n",
    "trusted_order_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingaia",
   "language": "python",
   "name": "ingaia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
